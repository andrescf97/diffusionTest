\documentclass[acmtog]{techreportacmart}

\usepackage{booktabs} % For formal tables


\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Copyright
\setcopyright{none}

\settopmatter{printacmref=false, printccs=false, printfolios=true}
\citestyle{acmauthoryear}
\setcitestyle{square}

% Document starts
\begin{document}
% Title portion
\title{A Physics-informed Diffusion Model for High-fidelity Flow Field Reconstruction} 
\author{Andrés Forero}
\affiliation{%
  \institution{Technical University of Munich}
}


\renewcommand\shortauthors{Forero}

\begin{abstract}
This report examines and extends the work of \cite{Shu23}, which introduces a physics-informed diffusion model for reconstructing high-fidelity computational fluid dynamics (CFD) data. The proposed framework formulates the super-resolution problem as one of data denoising, leveraging a Denoising Diffusion Probabilistic Model (DDPM) \cite{Ho20} to generate physically consistent flow fields from low-resolution or sparse inputs. Through a guided sampling process that aligns noisy low-fidelity data with the training distribution, the model achieves improved generalization without the need for paired datasets. Physical consistency is enforced via two complementary strategies: a learned encoding of the Navier–Stokes residual during training and a gradient-based correction during inference. 

Building on this foundation, the present study reproduces and analyzes the core methodology using a simplified test case of decaying two-dimensional turbulence. This additional experiment validates the adaptability of the diffusion framework to alternative flow regimes and assesses the comparative performance of the two physics-guidance strategies. The results confirm that explicit gradient-based correction enhances physical consistency in sparse or under-resolved cases, though at increased computational cost. A critical analysis of the diffusion approach highlights its strong generalization and physical fidelity but also identifies challenges related to efficiency, scalability, and uncertainty quantification. The report concludes by outlining future research directions, including accelerated sampling, integration of uncertainty estimation, and extensions to three-dimensional and non-periodic CFD domains.
\end{abstract}


%
% End generated code
%

\keywords{Computational fluid dynamics, diffusion models, physics-informed learning, turbulence reconstruction, uncertainty quantification}



\thanks{This report is a part of the lecture, Master-Seminar - Deep Learning in
  Computer Graphics, Informatics 15, Technical University of Munich.
  }


\maketitle


\section{Introduction}


This report examines the paper “A Physics-Informed Diffusion Model for High-Fidelity 
Flow Field Reconstruction” by Shu, Li, and Barati Farimani \cite{Shu23}, which presents 
a novel approach to improving the reconstruction of high-fidelity computational fluid dynamics 
(CFD) data. The authors propose a diffusion-based deep learning framework capable of generating 
physically consistent, high-resolution flow fields from low-resolution or sparsely sampled inputs. 
This work stands out for integrating physics-informed conditioning—derived from the Navier–Stokes 
equations into a denoising diffusion probabilistic model (DDPM), thereby enhancing both accuracy 
and generalization beyond what traditional data-driven methods can achieve.

The purpose of this report is to summarize the key ideas and contributions of the paper, compare 
the proposed diffusion-based framework with other machine learning techniques for CFD super-resolution, 
and critically assess its advantages, limitations, and potential future applications. By connecting 
the diffusion model’s probabilistic foundation with physical constraints, this work demonstrates a 
promising step toward scalable, reliable, and physics-aware data reconstruction in fluid mechanics.

% Head 1
\section{Summary of Paper}

% Head 2
\subsection{Problem Context}

High-fidelity Computational Fluid Dynamics (CFD) simulations are fundamental for analyzing how engineering systems interact with complex fluid flows. However, their computational cost increases rapidly with the Reynolds number, as Direct Numerical Simulations (DNS) \cite{Moin98} require solving the full Navier–Stokes equations in both space and time. This results in a long-standing trade-off between accuracy and computational efficiency. Lower-cost approaches such as Reynolds-Averaged Navier–Stokes (RANS) \cite{Launder74}, Large Eddy Simulations (LES) \cite{Pitsch06}, and hybrid RANS–LES models \cite{Shur08} introduce sub-grid scale models that estimate effective eddy viscosities using simplified assumptions \cite{Boffetta12}. While these models significantly reduce computational effort, they do so at the expense of fidelity, making it challenging to resolve fine-scale turbulent structures at high Reynolds numbers.

Recent developments in machine learning (ML) have attempted to bridge this gap by reconstructing high-fidelity CFD data from low-fidelity numerical or experimental inputs. Convolutional Neural Networks (CNNs), particularly U-Net architectures with skip connections and multi-scale filters, have demonstrated success in recovering high-resolution flow fields with strong quantitative metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) \cite{Pant20}. More advanced architectures, such as multi-scale enhanced super-resolution Generative Adversarial Networks (GANs) with physics-based losses \cite{Yousif21}, and CycleGANs trained on unpaired turbulence data \cite{Kim21}, have further improved reconstruction quality even without paired datasets. Other approaches focus on correcting numerical discretization errors using neural networks trained on Eulerian \cite{Zhumekenov19, Wang19, Pfaff20} or Lagrangian \cite{Ummenhofer19, Li22, SanchezGonzalez20, Ladicky15} representations of the flow.

Despite these advances, most ML-based super-resolution models suffer from limited generalization. They are trained on specific low-fidelity datasets and often fail when the input distribution during inference differs significantly from that of the training data. This dependency on paired, distribution-specific datasets limits scalability and real-world applicability: each new source of under-resolved or experimental data typically requires retraining. Consequently, there remains a need for a more generalizable, physics-consistent framework capable of reconstructing high-fidelity flow fields from diverse low-fidelity or sparse inputs without retraining.


\subsection{Methodology}
\label{subsec:methodology}

In contrast to conventional super-resolution networks that learn an explicit mapping from
low-fidelity (LF) to high-fidelity (HF) flow fields, the approach of Shu et al.~\cite{Shu23}
reformulates HF reconstruction as a \emph{conditional denoising problem} and solves it with a
Denoising Diffusion Probabilistic Model (DDPM)~\cite{Ho20}. The key idea is:

\begin{itemize}
    \item The diffusion model is trained \textbf{only on high-fidelity data} and learns to map
    \emph{noisy HF samples} back to clean HF samples.
    \item At inference time, a low-fidelity input is first transformed into a noisy sample that
    resembles the training distribution, and the DDPM then denoises it into a high-fidelity field.
\end{itemize}

This decouples the reconstruction problem from any specific low-fidelity generation process
(filtering, downsampling, sparsification) and makes the method more robust to changes in the
input distribution.

\subsubsection{Problem formulation as conditional denoising}

Let $\boldsymbol{x}_0 \in \mathcal{Y}$ denote a high-fidelity vorticity field from the Kolmogorov
flow dataset~\cite{Shu23}, and let $\boldsymbol{x}^{(g)} \in \mathcal{X}$ denote a low-fidelity
observation (e.g.\ uniformly downsampled, filtered, or sparse measurements on the grid).
Classical super-resolution methods explicitly learn a mapping
$f_\phi : \mathcal{X} \to \mathcal{Y}$ such that $f_\phi(\boldsymbol{x}^{(g)}) \approx \boldsymbol{x}_0$.
This coupling to a specific $\mathcal{X}$ makes them sensitive to distribution shifts in the
low-fidelity data.

Instead, the diffusion-based framework keeps the \emph{model} purely in the high-fidelity
space and uses the low-fidelity input only as a \emph{guide} for the denoising process.
Formally, the DDPM learns a family of conditional distributions
$p_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)$ on $\mathcal{Y}$, while the LF input
$\boldsymbol{x}^{(g)}$ is injected via the initialization of the reverse process at a
chosen time-step $t$.

\subsubsection{Forward diffusion process on high-fidelity data}

During training, only HF samples $\boldsymbol{x}_0 \sim p_{\text{data}}(\mathcal{Y})$ are used.
A forward (diffusion) process gradually corrupts $\boldsymbol{x}_0$ into Gaussian noise through
a Markov chain
\begin{equation}
    q(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}) :=
    \mathcal{N}\bigl(\boldsymbol{x}_t; \sqrt{\alpha_t}\,\boldsymbol{x}_{t-1},
    \beta_t \mathbf{I}\bigr),
    \qquad t = 1,\dots,T,
\end{equation}
where $\{\beta_t\}_{t=1}^T$ is a predefined variance schedule and $\alpha_t := 1 - \beta_t$.
This chain has the closed-form expression
\begin{equation}
    q(\boldsymbol{x}_t \mid \boldsymbol{x}_0) =
    \mathcal{N}\bigl(
        \boldsymbol{x}_t; \sqrt{\bar{\alpha}_t}\,\boldsymbol{x}_0,
        (1 - \bar{\alpha}_t)\mathbf{I}
    \bigr),
    \qquad
    \bar{\alpha}_t := \prod_{i=1}^t \alpha_i.
\end{equation}

Intuitively, $\boldsymbol{x}_t$ is a \emph{controlled mixture} of signal and noise:
\begin{equation}
    \boldsymbol{x}_t = \sqrt{\bar{\alpha}_t}\,\boldsymbol{x}_0
    + \sqrt{1 - \bar{\alpha}_t}\,\boldsymbol{\epsilon},
    \qquad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}).
\end{equation}
For small $t$, $\boldsymbol{x}_t$ is close to the original flow field, whereas for large $t$
it approaches pure Gaussian noise.

\subsubsection{Reverse process and DDPM training objective}

The generative model is defined by a reverse Markov chain
\begin{equation}
    p_\theta(\boldsymbol{x}_{0:T}) :=
    p(\boldsymbol{x}_T)\,
    \prod_{t=1}^T p_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t),
    \qquad
    p(\boldsymbol{x}_T) = \mathcal{N}(\mathbf{0}, \mathbf{I}),
\end{equation}
with Gaussian transitions
\begin{equation}
    p_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)
    := \mathcal{N}\bigl(
        \boldsymbol{x}_{t-1};
        \boldsymbol{\mu}_\theta(\boldsymbol{x}_t, t),
        \Sigma_t \mathbf{I}
    \bigr).
\end{equation}
Following Ho et al.~\cite{Ho20}, the mean $\boldsymbol{\mu}_\theta$ is parameterized in terms of a
learned \emph{noise predictor} $\boldsymbol{\epsilon}_\theta$:
\begin{equation}
    \boldsymbol{\mu}_\theta(\boldsymbol{x}_t, t) =
    \frac{1}{\sqrt{\alpha_t}}\left(
        \boldsymbol{x}_t -
        \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}}\,
        \boldsymbol{\epsilon}_\theta(\boldsymbol{x}_t, t)
    \right).
\end{equation}
The network $\boldsymbol{\epsilon}_\theta$ is implemented as a U-Net~\cite{Ronneberger15}, which
operates on vorticity fields and is conditioned on the discrete time-step $t$.

The model is trained by minimizing the simplified denoising objective
\begin{equation}
    \mathcal{L}_{\text{simple}}(\theta)
    = \mathbb{E}_{\boldsymbol{x}_0,\,t,\,\boldsymbol{\epsilon}}\left[
        \bigl\|
            \boldsymbol{\epsilon}
            - \boldsymbol{\epsilon}_\theta\bigl(
                \sqrt{\bar{\alpha}_t}\,\boldsymbol{x}_0
                + \sqrt{1 - \bar{\alpha}_t}\,\boldsymbol{\epsilon},
                t
            \bigr)
        \bigr\|_2^2
    \right],
\end{equation}
where $\boldsymbol{x}_0 \sim p_{\text{data}}(\mathcal{Y})$,
$t \sim \mathcal{U}\{1,\dots,T\}$ and
$\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0},\mathbf{I})$.
Thus, the DDPM learns to \emph{invert} the forward diffusion process by predicting the noise
that was added to a HF flow field at any time-step $t$.

\subsubsection{Conditional sampling with low-fidelity guidance}

At test time, the goal is to reconstruct a high-fidelity field from a low-fidelity observation
$\boldsymbol{x}^{(g)} \in \mathcal{X}$, without retraining the model for each new type of
low-fidelity data. Instead of starting the reverse chain from pure Gaussian noise
$\boldsymbol{x}_T \sim \mathcal{N}(\mathbf{0},\mathbf{I})$, Shu et al.~\cite{Shu23} perform
\emph{partial backward diffusion} starting from an intermediate time-step $t$ initialized by
a noisy version of the guidance input:
\begin{equation}
    \boldsymbol{x}_t
    = \sqrt{\bar{\alpha}_t}\,g(\boldsymbol{x}^{(g)})
    + \sqrt{1 - \bar{\alpha}_t}\,\boldsymbol{\epsilon}_t,
    \qquad
    \boldsymbol{\epsilon}_t \sim \mathcal{N}(\mathbf{0},\mathbf{I}),
    \label{eq:guided_init}
\end{equation}
where $g(\cdot)$ denotes a preprocessing operator that lifts the low-fidelity observation
to the HF grid (e.g.\ interpolation or nearest-neighbor padding for sparse data).
Starting from $\boldsymbol{x}_t$, a DDPM/DDIM-style reverse process
(e.g.\ the implicit sampler of Song et al.~\cite{Song20DDIM}) is run from $t$ down to $0$:
\begin{equation}
    \boldsymbol{x}_{t-1}
    \sim p_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t),
    \quad \dots,\quad
    \boldsymbol{x}_0 \sim p_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1).
\end{equation}
The final state $\boldsymbol{x}_0$ is taken as the reconstructed high-fidelity flow field.

\subsubsection{Noise-induced distribution alignment and generalization}

The crucial effect of the initialization in~\eqref{eq:guided_init} is that it maps diverse
low-fidelity inputs into the \emph{same noisy high-fidelity manifold} that the DDPM was
trained on. During training, all inputs to $\boldsymbol{\epsilon}_\theta$ have the form
\[
    \boldsymbol{x}_t^{\text{train}}
    = \sqrt{\bar{\alpha}_t}\,\boldsymbol{x}_0
    + \sqrt{1 - \bar{\alpha}_t}\,\boldsymbol{\epsilon},
    \qquad \boldsymbol{x}_0 \sim p_{\text{data}}(\mathcal{Y}).
\]
At inference, after preprocessing and noise injection, the guided initialization
$\boldsymbol{x}_t^{\text{test}}$ in~\eqref{eq:guided_init} has the \emph{same statistical form},
regardless of how $\boldsymbol{x}^{(g)}$ was generated (different filters, resolutions, or
sparsity patterns). By choosing $t$ appropriately, the model ensures that:
\begin{itemize}
    \item the signal component $\sqrt{\bar{\alpha}_t}\,g(\boldsymbol{x}^{(g)})$ carries the
    large-scale structure of the low-fidelity observation, while
    \item the noise component $\sqrt{1 - \bar{\alpha}_t}\,\boldsymbol{\epsilon}_t$ pushes
    the sample into the region of the space where the DDPM has learned a reliable denoising
    mapping.
\end{itemize}

In other words, the model is \textbf{not} trained to map low-fidelity $\to$ high-fidelity.
It is trained to map \emph{noisy high-fidelity-like inputs} to clean high-fidelity fields.
By first transforming the low-fidelity input into such a noisy sample, the reverse diffusion
process can be reused for a wide range of low-fidelity data distributions without retraining.
This noise-induced alignment is the main mechanism behind the improved robustness and
generalization reported in~\cite{Shu23}.


\subsubsection{Implementation}

The proposed approach is evaluated on two-dimensional Kolmogorov flow, governed by the incompressible Navier–Stokes equations in vorticity form. The simulations are performed at a Reynolds number of 1000 under periodic boundary conditions, with a forcing term 
\[
f(x) = -4\cos(4x_2) - 0.1\,\omega(x,t),
\]
which maintains turbulence while preventing excessive energy accumulation. The dataset consists of 40 simulated sequences of 10 seconds each, generated on a $2048 \times 2048$ grid and downsampled to $256 \times 256$. The first 36 sequences are used for training, while the remaining four are reserved for testing. 

The model operates on vorticity fields from three consecutive frames to capture temporal derivatives, and the PDE residuals are computed using Fourier transforms for spatial derivatives and finite differences for temporal derivatives. This combination allows accurate evaluation of the convection and diffusion terms, while maintaining numerical stability and ensuring that the reconstructed flow fields respect the underlying physical dynamics.

\bigskip

Through this framework, \cite{Shu23} demonstrate that their diffusion-based approach can reconstruct high-fidelity flow fields from both low-resolution and sparsely sampled inputs with strong accuracy and generalization. By combining probabilistic denoising with physics-informed conditioning, the model bridges the gap between data-driven learning and physical laws, providing a scalable and robust solution for CFD super-resolution.

\section{Experimental Setup}

To further explore the generalization capability of physics-informed diffusion models, a smaller-scale experiment was implemented using a simplified flow configuration.  
The chosen test case is the \textbf{decaying two-dimensional turbulence} problem governed by the incompressible Navier–Stokes equations in vorticity form:

\[
\frac{\partial \omega}{\partial t} + \mathbf{u}\cdot\nabla \omega = \frac{1}{Re}\nabla^2 \omega, 
\quad 
\nabla\cdot\mathbf{u} = 0,
\quad 
\mathbf{u} = \nabla^\perp \psi,\ \psi = \nabla^{-2}\omega.
\]

No external forcing is applied, and the flow energy naturally decays over time.  
This configuration maintains the nonlinear vortex interactions characteristic of turbulence while simplifying boundary conditions and external inputs.

\paragraph{Dataset.}
A pseudo-spectral solver implemented in PyTorch was used to simulate the flow under periodic boundary conditions on a square domain $(0,2\pi)^2$ with a Reynolds number of $Re = 1000$.  
The high-fidelity data were generated on a $256\times256$ grid and downsampled to $64\times64$ and $32\times32$ to form the low-fidelity inputs.  
A total of 20 temporal sequences of 5 seconds each were simulated with a time step of $\Delta t = 1/32$ s, yielding 160 frames per sequence.  
The dataset was split into 16 sequences for training and 4 for testing.

\paragraph{Preprocessing.}
Each training sample consists of three consecutive vorticity frames $[\omega_{t-1}, \omega_t, \omega_{t+1}]$ used to approximate temporal derivatives.  
Spatial derivatives were computed in Fourier space, and the PDE residual
\[
r = \partial_t \omega + \mathbf{u}\cdot\nabla \omega - \tfrac{1}{Re}\nabla^2\omega
\]
was used to evaluate physical consistency.  
The gradient of the residual, $c = \nabla_{x_t}r$, served as the physics-guidance signal for both training and inference.

\paragraph{Model Variants.}
Three model configurations were evaluated:
\begin{enumerate}
    \item \textbf{Baseline DDPM:} trained purely on data without physics guidance.
    \item \textbf{Physics-Informed (Strategy 1):} residual gradient encoded as an additional conditioning input.
    \item \textbf{Physics-Informed (Strategy 2):} gradient-descent correction term applied during the sampling process.
\end{enumerate}
A standard U-Net-based super-resolution model was trained as a reference baseline to compare reconstruction quality and generalization.

\paragraph{Inference Procedure.}
Low-fidelity inputs were mixed with Gaussian noise to produce guided noisy samples:
\[
x_t = \sqrt{\bar{\alpha}_t}x^{(g)} + \sqrt{1-\bar{\alpha}_t}\epsilon_t,
\]
where $x^{(g)}$ represents the downsampled or sparse flow field and $\epsilon_t \sim \mathcal{N}(0,I)$.  
The denoising chain was then applied iteratively to reconstruct high-fidelity outputs, optionally using the physics correction term from Strategy 2:
\[
x_{t-1} = x_{t-1}^{(DDIM)} - \lambda \nabla_x \|G(x_t)\|^2.
\]
For extremely sparse inputs, iterative refinement ($K=3$ cycles) was employed until convergence.

\paragraph{Evaluation.}
Model performance was assessed using quantitative metrics such as Root Mean Square Error (RMSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index (SSIM), as well as spectral comparisons of kinetic energy and vorticity fields to evaluate physical realism.

\subsection{Results and Discussion}
\textbf{[To be completed.]}
\vspace{1cm}
% TODO: Insert figures and quantitative comparisons here
% TODO: Add discussion on model generalization, spectral accuracy, and physics consistency
\vspace{1cm}

\section{Conclusions}

\subsection{Summary of Contributions}

The work of \cite{Shu23} presents a diffusion-based deep learning framework that merges generative probabilistic modeling with physical principles to reconstruct high-fidelity computational fluid dynamics (CFD) data. The study reformulates the super-resolution problem as a denoising task through a Denoising Diffusion Probabilistic Model (DDPM) \cite{Ho20}, where high-fidelity flow fields are progressively corrupted with Gaussian noise during training and then reconstructed through a learned reverse process. This formulation allows the model to generate physically consistent high-fidelity data from low-resolution or sparsely sampled inputs without the need for paired training datasets. 

A central innovation in this approach lies in the guided sampling process, which introduces a noise-mixing mechanism to align low-fidelity inference data with the noisy high-fidelity samples seen during training. This improves model generalization across diverse input distributions, addressing one of the key weaknesses of previous CNN- and GAN-based super-resolution methods. Furthermore, the model integrates physical knowledge through two physics-informed strategies: one that encodes the residual of the governing Navier–Stokes equations as additional conditioning input during training, and another that applies a gradient-based correction term to the sampling process during inference. Both mechanisms ensure that reconstructed fields not only reproduce the spatial and temporal statistics of the training data but also satisfy the underlying fluid dynamics equations. The method was validated using two-dimensional Kolmogorov flow at a Reynolds number of 1000, demonstrating improved reconstruction accuracy and generalization compared to baseline deep learning models.

\subsection{Advantages and Scientific Impact}

The diffusion-based framework proposed by \cite{Shu23} constitutes a significant methodological advance in the application of machine learning to CFD. One of its major strengths is its ability to generalize to unseen input distributions. By training exclusively on high-fidelity data and introducing noise mixing during inference, the model can handle varying levels of data sparsity and resolution without retraining. This decoupling of the model from a specific low-fidelity distribution makes it far more adaptable than conventional CNN or GAN architectures, which typically overfit to a particular dataset.

Another central advantage of the proposed framework is its explicit incorporation of physical laws. The use of the Navier–Stokes residual as a corrective signal constrains the denoising trajectory to follow physically valid states, reducing unphysical artifacts and improving the spectral coherence of the reconstructed flow fields. The DDPM formulation also ensures stable optimization, avoiding the convergence issues and mode collapse commonly encountered in adversarial learning. Furthermore, as a probabilistic generative model, the diffusion process inherently allows for uncertainty estimation by sampling multiple plausible reconstructions, paving the way for future work in uncertainty quantification within fluid mechanics. 

Finally, the architecture’s modularity makes it a promising foundation for integration with other deep learning paradigms. The model can be coupled with transformer-based encoders, graph neural networks, or even embedded within numerical solvers for hybrid CFD–ML systems. In this sense, the diffusion framework provides a flexible bridge between physics-based simulation and data-driven modeling, offering a structured path toward generalizable and physically grounded fluid reconstruction.

\subsection{Limitations and Open Challenges}

Despite its conceptual strengths, several challenges remain before diffusion-based CFD models can be adopted in large-scale scientific or engineering workflows. The most immediate limitation is computational cost. Because the reverse diffusion process requires hundreds of iterative denoising steps, inference times are significantly longer than those of single-pass neural surrogates or reduced-order models. Although this iterative nature enhances stability, it limits the model’s practicality for real-time or high-resolution three-dimensional applications. 

The current framework is also constrained by its reliance on two-dimensional periodic turbulence. While this setting offers a controlled benchmark, it does not capture the full complexity of real-world CFD problems involving non-periodic boundaries, varying Reynolds numbers, or multiphysics interactions. Extending the approach to three-dimensional or irregular domains would require both algorithmic and architectural adaptations, particularly because the Fourier-based residual computation assumes uniform periodic grids. This assumption simplifies derivative estimation but prevents direct application to geometrically complex meshes or boundary-layer problems.

Another limitation concerns the sensitivity of the physics–data coupling term. The balance between learned data-driven reconstruction and PDE residual correction, controlled by the parameter $\lambda$, must be carefully tuned. Excessive weighting toward the physics term can distort the denoising trajectory, while insufficient weighting fails to enforce physical constraints. Additionally, while the diffusion model is inherently probabilistic, the study does not explicitly quantify uncertainty in its predictions. The lack of calibrated uncertainty measures limits its interpretability and prevents its use in safety-critical or design-sensitive CFD applications. 

Recent works have highlighted similar issues. Benchmark studies of diffusion-based surrogate models \cite{2408.11104v3} show that while these models achieve remarkable stability and physical fidelity, their inference times remain high, and their uncertainty quantification capabilities are underdeveloped. Related research on diffusion-based surrogates for airfoil flows \cite{liu-thuerey-2024-uncertainty-aware-surrogate-models-for-airfoil-flow-simulations-with-denoising-diffusion-probabilistic} confirms that improved uncertainty awareness and efficient sampling strategies are necessary for the diffusion framework to transition from a research concept to a practical modeling tool.

\subsection{Future Work and Research Directions}

Future work should focus on expanding the scalability, efficiency, and interpretability of physics-informed diffusion models for CFD. One key direction is extending the framework to three-dimensional turbulence and geometrically complex domains. This could be achieved using latent-space diffusion models or conditional neural fields that reduce the computational burden while preserving physical accuracy. A second direction involves accelerating the sampling process. Techniques such as model distillation, learned time-step scheduling, or single-step approximations could significantly reduce inference time without sacrificing accuracy, addressing one of the major bottlenecks in applying diffusion models to large-scale CFD.

In parallel, integrating explicit uncertainty quantification would enhance the interpretability of diffusion-based surrogates. Methods that estimate prediction confidence or generate calibrated uncertainty maps could improve decision-making in engineering and scientific applications. Furthermore, the reliance on Fourier-based residual estimation could be mitigated by adopting discretization-agnostic physics operators or differentiable numerical solvers, allowing the model to generalize to non-periodic domains. Finally, coupling the diffusion process directly within numerical solvers would enable on-the-fly correction of coarse simulations, leading to hybrid CFD–ML frameworks capable of dynamically balancing fidelity and computational cost.

Recent studies already point toward these directions. Latent diffusion and diffusion-transformer models have demonstrated improved efficiency and scalability in complex flow domains, while uncertainty-aware diffusion surrogates have shown promise in quantifying model confidence and detecting failure modes. Building on these advances, future research could make physics-informed diffusion models a standard component of next-generation CFD workflows.

\subsection{Reflections on the Implemented Experiment}

As part of this study, a supplementary experiment was implemented to test the adaptability of the diffusion framework on a simpler but physically rich system: decaying two-dimensional turbulence. This setup preserved the nonlinear dynamics characteristic of turbulent flows while removing external forcing, providing an ideal environment for testing generalization. The experiment reproduced both of the physics-informed guidance strategies introduced by \cite{Shu23}. The learned residual encoding (Strategy 1) provided smooth convergence and consistent reconstructions, while the gradient-descent correction applied during inference (Strategy 2) yielded better physical consistency, particularly in terms of spectral alignment with reference data. 

These findings suggest that explicit enforcement of physical constraints during inference can effectively compensate for data limitations, even in settings with sparse or noisy inputs. However, computational cost remains a limiting factor, and further optimization such as reducing the number of denoising steps or introducing uncertainty estimation would be necessary for practical use. Overall, the results reaffirm the potential of physics-informed diffusion models to unify physical accuracy, probabilistic modeling, and machine learning in CFD, while also highlighting the need for efficiency and interpretability improvements in future research.


% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-bibliography}



\end{document}
